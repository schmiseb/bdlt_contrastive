{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import re\n",
    "import xml.dom.minidom\n",
    "from collections import namedtuple\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import random\n",
    "\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/pan-webis-de/pan-code/blob/master/clef13/text-alignment/pan13-text-alignment-eval.py\n",
    "\n",
    "TREF, TOFF, TLEN = 'this_reference', 'this_offset', 'this_length'\n",
    "SREF, SOFF, SLEN = 'source_reference', 'source_offset', 'source_length'\n",
    "EXT = 'is_external'\n",
    "Annotation = namedtuple('Annotation', [TREF, TOFF, TLEN, SREF, SOFF, SLEN, EXT])\n",
    "TREF, TOFF, TLEN, SREF, SOFF, SLEN, EXT = range(7)\n",
    "\n",
    "def extract_annotations_from_file(xmlfile, tagname):\n",
    "    \"\"\"Returns a set of plagiarism annotations from an XML file.\"\"\"\n",
    "    doc = xml.dom.minidom.parse(xmlfile)\n",
    "    annotations = set()\n",
    "    if not doc.documentElement.hasAttribute('reference'):\n",
    "        return annotations\n",
    "    t_ref = doc.documentElement.getAttribute('reference')\n",
    "    for node in doc.documentElement.childNodes:\n",
    "        if node.nodeType == xml.dom.Node.ELEMENT_NODE and \\\n",
    "           node.hasAttribute('name') and \\\n",
    "           node.getAttribute('name').endswith(tagname):\n",
    "            ann = extract_annotation_from_node(node, t_ref)\n",
    "            if ann:\n",
    "                annotations.add(ann)\n",
    "    return annotations\n",
    "\n",
    "\n",
    "def extract_annotation_from_node(xmlnode, t_ref):\n",
    "    \"\"\"Returns a plagiarism annotation from an XML feature tag node.\"\"\"\n",
    "    if not (xmlnode.hasAttribute('this_offset') and \\\n",
    "            xmlnode.hasAttribute('this_length')):\n",
    "        return False\n",
    "    t_off = int(xmlnode.getAttribute('this_offset'))\n",
    "    t_len = int(xmlnode.getAttribute('this_length'))\n",
    "    s_ref, s_off, s_len, ext = '', 0, 0, False\n",
    "    if xmlnode.hasAttribute('source_reference') and \\\n",
    "       xmlnode.hasAttribute('source_offset') and \\\n",
    "       xmlnode.hasAttribute('source_length'):\n",
    "        s_ref = xmlnode.getAttribute('source_reference')\n",
    "        s_off = int(xmlnode.getAttribute('source_offset'))\n",
    "        s_len = int(xmlnode.getAttribute('source_length'))\n",
    "        ext = True\n",
    "    return Annotation(t_ref, t_off, t_len, s_ref, s_off, s_len, ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_substring(file, offset, length):\n",
    "    with open(file, encoding='utf-8') as file:\n",
    "        if not length == 0:\n",
    "            return file.read()[offset:offset + length]\n",
    "        else:\n",
    "            return file.read()\n",
    "\n",
    "def split_in_sentences(text):\n",
    "    text = ' '.join(text.split())\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "def get_string_without_paraphrase(file, offset, length):\n",
    "    with open(file, encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    pre = text[0:offset]\n",
    "    post = text[offset+length:]\n",
    "    return pre + post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpurs_pre = \"pan13-text-alignment-test-\"\n",
    "corpus_append = \"corpus1\"\n",
    "corpus_dir = corpurs_pre + corpus_append + \"/\"\n",
    "subdirs = [\"02-no-obfuscation\", \"03-random-obfuscation\", \"04-translation-obfuscation\"]\n",
    "\n",
    "for dir in subdirs:\n",
    "    out_dirs = []\n",
    "    all_xmls = glob(corpus_dir + dir + \"/*.xml\")\n",
    "    xmls_dicts = []\n",
    "    annotation_number = 0\n",
    "    for file in sorted(all_xmls):\n",
    "        annotations = extract_annotations_from_file(file, \"plagiarism\")\n",
    "        for ann in annotations:\n",
    "            source_text = get_substring(corpus_dir + \"src/\" + ann[SREF], ann[SOFF], ann[SLEN])\n",
    "            sus_text = get_substring(corpus_dir + \"susp/\" + ann[TREF], ann[TOFF], ann[TLEN])\n",
    "            source_text_no_para = get_string_without_paraphrase(corpus_dir + \"src/\" + ann[SREF], ann[SOFF], ann[SLEN])\n",
    "            sentences_source = split_in_sentences(source_text)\n",
    "            sentences_sus = split_in_sentences(sus_text)\n",
    "            sentences_source_no_para = split_in_sentences(source_text_no_para)\n",
    "            for src in sentences_source:\n",
    "                for sus in sentences_sus:\n",
    "                    d = {\n",
    "                        \"annotation_number\": annotation_number,\n",
    "                        \"sentence1\": src,\n",
    "                        \"sentence2\": sus,\n",
    "                        \"label\":1,\n",
    "                        \"source\": ann[SREF],\n",
    "                        \"suspicion\": ann[TREF]\n",
    "                    }\n",
    "                    out_dirs.append(d)\n",
    "                \n",
    "            try:\n",
    "                sentences_source_no_para = random.sample(sentences_source_no_para, 5)\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "            for src in sentences_source_no_para:\n",
    "                for sus in sentences_sus:\n",
    "                    d = {\n",
    "                        \"annotation_number\": annotation_number,\n",
    "                        \"sentence1\": src,\n",
    "                        \"sentence2\": sus,\n",
    "                        \"label\":0,\n",
    "                        \"source\": ann[SREF],\n",
    "                        \"suspicion\": ann[TREF]\n",
    "                    }\n",
    "                    out_dirs.append(d)\n",
    "            annotation_number += 1\n",
    "    pd.DataFrame(out_dirs).to_csv(corpus_append + \"-\" + dir + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpurs_pre = \"pan13-text-alignment-test-\"\n",
    "corpus_append = \"corpus2\"\n",
    "corpus_dir = corpurs_pre + corpus_append + \"/\"\n",
    "subdirs = [\"01-no-plagiarism\"]\n",
    "\n",
    "for dir in subdirs:\n",
    "    out_dirs = []\n",
    "    all_xmls = glob(corpus_dir + dir + \"/*.xml\")\n",
    "    xmls_dicts = []\n",
    "    annotation_number = 0\n",
    "    for file in sorted(all_xmls):\n",
    "        source = re.search('source-document\\d*', file).group( )\n",
    "        suspicion = re.search('suspicious-document\\d*', file).group()\n",
    "        source_text = get_substring(corpus_dir + \"src/\" + source + '.txt', 0, 0)\n",
    "        sus_text = get_substring(corpus_dir + \"susp/\" + suspicion + '.txt', 0, 0)\n",
    "        sentences_source = split_in_sentences(source_text)\n",
    "        sentences_sus = split_in_sentences(sus_text)\n",
    "        try:\n",
    "            sentences_source = random.sample(sentences_source, 10)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            sentences_sus = random.sample(sentences_sus, 5)\n",
    "        except:\n",
    "            pass\n",
    "        for src in sentences_source:\n",
    "            for sus in sentences_sus:\n",
    "                d = {\n",
    "                    \"annotation_number\": annotation_number,\n",
    "                    \"sentence1\": src,\n",
    "                    \"sentence2\": sus,\n",
    "                    \"label\":0,\n",
    "                    \"source\": source + '.txt',\n",
    "                    \"suspicion\": suspicion + '.txt'\n",
    "                }\n",
    "                out_dirs.append(d)\n",
    "        annotation_number += 1\n",
    "    pd.DataFrame(out_dirs).to_csv(corpus_append + \"-\" + dir + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('bdlt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "93a0b903a60a7bb54103f7d1330837582c76a787345912a3be1026133156e7b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
