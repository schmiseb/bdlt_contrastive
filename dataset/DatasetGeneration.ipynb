{"cells":[{"cell_type":"markdown","source":["# 1. Preparation"],"metadata":{"id":"J_V6G4LUhwrA"}},{"cell_type":"markdown","source":["## 1.1 Mount drive"],"metadata":{"id":"KSaEj8UXkjv8"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"Uu8RafSARvNO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660230915229,"user_tz":-120,"elapsed":3834,"user":{"displayName":"Jonas Probst","userId":"12529974203254550175"}},"outputId":"18deb888-937f-4dd6-bd2c-150afdfa2eb8"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!cp -r /content/drive/MyDrive/Uni\\ Leipzig/SS2022/.ssh ~/"],"metadata":{"id":"PXf5iED29aeo","executionInfo":{"status":"ok","timestamp":1660230915229,"user_tz":-120,"elapsed":4,"user":{"displayName":"Jonas Probst","userId":"12529974203254550175"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fE3snvUmbw97"},"source":["## 1.2 Get correct CuDNN version\n","See https://github.com/googlecolab/colabtools/issues/2600\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-pbxWrx2eUE2"},"outputs":[],"source":["!apt install --allow-change-held-packages libcudnn8=8.4.1.50-1+cuda11.6"]},{"cell_type":"markdown","metadata":{"id":"pO9PjgdF-cXV"},"source":["## 1.3 Install necessary packages"]},{"cell_type":"code","source":["%cd '/content/drive/MyDrive/Uni Leipzig/SS2022/bdlt_contrastive/dataset'"],"metadata":{"id":"KlyQDWNmLcK1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install git+https://github.com/PrithivirajDamodaran/Parrot_Paraphraser.git"],"metadata":{"id":"-KCxApQZ5RdB"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ui8nCZIa-bvM"},"outputs":[],"source":["!python3 -m pip install -r '../requirements.txt'"]},{"cell_type":"code","source":["import re\n","\n","def para(model, tokenizer, row, anchor_col=\"sentence1\", para_col=\"sentence2\", label_col=\"label\", path=\"glue\", name=\"mrpc\", split=\"train\"):\n","  out_row = {\n","             \"anchor\": row[anchor_col],\n","             \"para\": row[para_col],\n","             \"neg_1\": \"\",\n","             \"original_label\": row[label_col],\n","             \"contains_numeric\": False,\n","             \"path\": path,\n","             \"name\": name,\n","             \"split\": split}\n","\n","  # return samples which contain paraphrase as is\n","  if row[label_col] == 1:\n","        return out_row\n","\n","\n","  # move hard negative to neg_1\n","  out_row[\"neg_1\"] = row[para_col]\n","  #mark rows with digits in anchor sentence, leads to logical errors in paraphrase\n","  if re.search(\"\\d\", row[anchor_col]):\n","        out_row[\"contains_numeric\"] = True\n","\n","\n","  anchor = row[anchor_col]\n","  anchor = re.sub(r'\\s([\\.,!\\?\\'\"])', r'\\1', anchor)\n","  anchor = anchor.replace('\" ', '\"')\n","\n","\n","  encoding = tokenizer.encode(\"paraphrase: \" + anchor, return_tensors=\"pt\")\n","  input_ids = encoding.to(device)\n","\n","\n","  paraphrases = model.generate(\n","      input_ids=input_ids,\n","      max_length=128,\n","      early_stopping=True,\n","      num_beams=3,\n","      num_return_sequences=1\n","\n","  )\n","  \t\n","  para = tokenizer.decode(paraphrases[0], skip_special_tokens=True,clean_up_tokenization_spaces=True)\n","\n","  # remove first tokens from anchor and paraphrase, that contain \"paraphrsedoutput: \" and \"paraphrase: \"\n","  para_token_list =paraphrases[0].tolist()[7:]\n","  input_token_list = input_ids.tolist()[0][3:]\n","\n","  # check if generated tokens are the same as in original sentence, which means sentence was not altered or just shuffeled.\n","  # Sentences like this are not used\n","  # remove paraphrases without new tokens and empty ones\n","  if not para or sorted(para_token_list) ==sorted(input_token_list) or len(para_token_list) == 128:\n","    out_row[\"para\"] = \"None\"\n","    return out_row\n","  else:\n","      para = para.replace(\"paraphrasedoutput: \", \"\")\n","      out_row[\"para\"] = para\n","    \n","  return out_row"],"metadata":{"id":"hCgATXCZXoze","executionInfo":{"status":"ok","timestamp":1660236642595,"user_tz":-120,"elapsed":428,"user":{"displayName":"Jonas Probst","userId":"12529974203254550175"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","import torch\n","import datasets\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = AutoModelForSeq2SeqLM.from_pretrained(\"ramsrigouthamg/t5-large-paraphraser-diverse-high-quality\")\n","tokenizer = AutoTokenizer.from_pretrained(\"ramsrigouthamg/t5-large-paraphraser-diverse-high-quality\")\n","model.to(device)\n","print(\" \")\n"],"metadata":{"id":"_sFVm3Wu6w75","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660236677649,"user_tz":-120,"elapsed":32528,"user":{"displayName":"Jonas Probst","userId":"12529974203254550175"}},"outputId":"05085770-cffd-4d70-e88a-44647df0fa50"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":[" \n"]}]},{"cell_type":"code","source":["import pandas as pd\n","dataset_definitions = [\n","    #{\"path\": \"glue\", \"name\": \"mrpc\"},\n","    {\"path\": \"paws\", \"name\": \"labeled_final\"},\n","    {\"path\": \"paws\", \"name\": \"labeled_swap\"},\n","    {\"path\": \"paws\", \"name\": \"unlabeled_final\"}\n","]\n","out = {\"train\":[], \"validation\":[], \"test\":[]}\n","for definition in dataset_definitions:\n","  dataset = datasets.load_dataset(path=definition[\"path\"], name=definition[\"name\"])\n","  for split in [\"train\", \"validation\", \"test\"]:\n","    if split not in dataset:\n","      continue\n","    dataset_part = pd.DataFrame(dataset[split])\n","    #dataset_post = dataset_part.map(lambda x: para(model, tokenizer, x), num_proc=1)\n","    for idx, row in dataset_part.iterrows():\n","      out[split].append(para(model, tokenizer,row, split=split, path=definition[\"path\"], name=definition[\"name\"]))\n","for key, value in out.items():\n","    df = pd.DataFrame(value)\n","    df = df[df[\"para\"] != \"None\"]\n","    df = df.reset_index()\n","    df.to_csv(f\"out_{key}_paws.csv\", index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0,"referenced_widgets":["aebe1edaa40448ec9606c4fc9512a35e","8b5547893d0544719b1785d27bed3952","ba14bd923d8e4a65ae81712557f52e96","8472ee5390914e2cb55d85728c8b7e98","c66c70f66e374636ad1b6206bd76f98b","25caf84122e0457d8a8f46f9f4324cda","3e7910370d4f43b98db41ba43fea7d10","08a39228f7334bdfa334bc0e2da9f4aa","c5d306bdea5542d3981c353706395aec","d9872e9704c8499fa0a0e4d2777caed3","d6664fddb2a1444d813500d58236663a"]},"id":"77mHxTyfiIwq","outputId":"d728d177-0680-422a-c9f6-0be66fd8147f"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:datasets.builder:Reusing dataset paws (/root/.cache/huggingface/datasets/paws/labeled_final/1.1.0/8d567c6472623f42bd2cc635cad06932d0f0cd2f897db56013c1180f4317d338)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aebe1edaa40448ec9606c4fc9512a35e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}]},{"cell_type":"code","source":["for split in [\"train\", \"validation\", \"test\"]:\n","    anchors_with_para = pd.read_csv(f\"out_{split}_paws.csv\", index_col=\"index\").fillna(\"\")\n","\n","    df = {\"docno\": [], \"text\": []}\n","    for index, row in anchors_with_para.iterrows():\n","        for sent in [\"anchor\", \"para\", \"neg_1\"]:\n","            if sent and sent.strip() !=\"\" and sent != \"None\":\n","                df[\"text\"].append(row[sent])\n","                df[\"docno\"].append(str(index) + \"_\" + sent)\n","    df = pd.DataFrame.from_dict(df)\n","    print(df)\n","    # index the text, record the docnos as metadata\n","    pd_indexer = pt.DFIndexer(f\"./pd_index_{split}\")\n","    indexref = pd_indexer.index(df[\"text\"], df[\"docno\"])\n","\n","    batch_ret = pt.BatchRetrieve(indexref,num_results=10)\n","    out_df = {\"anchor\":[], \"para\":[], \"neg1\":[], \"neg2\":[], \"neg3\":[], \"neg4\":[], \"neg5\":[], \"neg6\":[]}\n","    for idx_row,row in anchors_with_para.iterrows():\n","        out_df[\"anchor\"].append(row[\"anchor\"])\n","        out_df[\"para\"].append(row[\"para\"])\n","        \n","        if row[\"neg_1\"] != \"\":\n","            neg_count = 2\n","            out_df[\"neg1\"].append(row[\"neg_1\"])\n","        else:\n","            neg_count=1\n","\n","        s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', row[\"anchor\"])\n","\n","        res = batch_ret(s)\n","        if res.shape[0] < 10:\n","            print(res)\n","        for res_num ,r in res.iterrows():\n","            idx_num, sent = r[\"docno\"].split(\"_\", 1)[0],  r[\"docno\"].split(\"_\", 1)[1]\n","            if int(idx_num) != idx_row:\n","                out_df[f\"neg{neg_count}\"].append(anchors_with_para.loc[int(idx_num)][sent])\n","                neg_count += 1\n","            if neg_count == 7:\n","                break\n","        if neg_count < 7:\n","            for n in range(neg_count, 7):\n","                out_df[f\"neg{n}\"].append(\"\")\n","\n","    out_df = pd.DataFrame(out_df)\n","    out_df.to_csv(f\"out_{split}.csv\")"],"metadata":{"id":"2O0Oe7gg1bo4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import datasets\n","import pandas as pd\n","import csv\n","import numpy as np\n","from random import shuffle\n","from parrot import Parrot\n","import torch\n","from nltk.util import ngrams\n","import nltk\n","from nltk.corpus import stopwords\n","\n","# =============================================================\n","# Hard negative mining\n","# =============================================================\n","class HardNegativePreparer():\n","    \"\"\"\n","    This class is used by the contrastive learning_manager to get negative samples for specified sentences.\n","    It takes the sentences from the csv \"Negative_Sentences.csv\" created by the HardNegativeFinder\n","    \"\"\"\n","    def __init__(self, csv_path=\"Negative_Sentences_Sig300.csv\"):\n","        self.df = pd.read_csv(csv_path)\n","\n","    def build_dataset_with_negatives(self, dataset: datasets.Dataset, n=1):\n","        \"\"\"\n","        :param dataset: Dataset with two sentences and a label\n","        :param n:       Number of negatives to add\n","        :return:        Dataset that only contains the pairs that have a label of 1.\n","                        Additionally, n more sentences were added\n","        \"\"\"\n","        # Filter only positive samples\n","        filtered_ds = dataset.filter(lambda x: x[\"label\"] == 1)\n","\n","        # Add the hard negatives to each ds separately\n","        train_ds = filtered_ds[\"train\"].map(lambda x: self.get_hard_negatives(sen_id=x[\"idx\"], n=n, dataset=\"train\"))\n","        val_ds = filtered_ds[\"validation\"].map(lambda x: self.get_hard_negatives(sen_id=x[\"idx\"], n=n, dataset=\"val\"))\n","        test_ds = filtered_ds[\"test\"].map(lambda x: self.get_hard_negatives(sen_id=x[\"idx\"], n=n, dataset=\"test\"))\n","\n","        # Combine the three datasets into one DatasetDict and return it\n","        return datasets.DatasetDict({\"train\": train_ds, \"validation\": val_ds, \"test\": test_ds})\n","\n","    def get_hard_negatives(self, sen_id: int, n=1, dataset=\"train\"):\n","        \"\"\"\n","        Takes in a sentence i and returns n hard negatives as obtained by the HardNegativeFinder\n","        :param sen_id:  The id for which to return hard negatives\n","        :param n:       The number of hard negatives to return\n","        :param dataset: The name of the dataset the sentence belongs to\n","        :return:        A dictionary of hard negative sentences (starting with sentence3, sentence4, ...)\n","        \"\"\"\n","     \n","        # Define the columns of self.df based on n\n","        columns = [str(\"sen_\" + str(idx)) for idx in range(n)]\n","\n","        # Get the correct row by matching the sen_id on anchor_idx and the dataset name\n","        row = self.df.loc[((self.df[\"anchor_idx\"] == \"sen1_\" + str(sen_id)) & (self.df[\"dataset\"]==dataset))]\n","\n","        # Get the sentences as a list and turn them into a dictionary starting with key sentence3\n","        sentences = row[columns].values.tolist()[0]\n","        result_dict = dict(zip([\"sentence\" + str(idx + 3) for idx in range(n)], sentences))\n","        return result_dict\n","    \n","\n","    def build_dataset_with_positives_and_negatives(self, dataset: datasets.Dataset, n=1):\n","        \"\"\"\n","        :param dataset: Dataset with two sentences and a label\n","        :param n:       Number of negatives to add\n","        :return:        Augmented Dataset. Negative labeled Sentences get one positive paraphrase added and are filled to reach n negatives\n","                        For already positive pairs, only negatives are created.\n","        \"\"\"\n","        # Filter only negative samples, for which positive is created\n","        #filtered_ds = dataset.filter(lambda x: x[\"label\"] == 0)\n","\n","        augmented_with_paraphrases = generate_paraphrases(dataset)\n","        augmented_with_paraphrases.to_csv(\"dataset_with_para.csv\")\n","        return\n","\n","\n","        # Add the hard negatives to each ds separately\n","        train_ds = augmented_with_paraphrases[\"train\"].map(lambda x: self.get_hard_negatives(sen_id=x[\"idx\"], n=n, dataset=\"train\"))\n","        val_ds = augmented_with_paraphrases[\"validation\"].map(lambda x: self.get_hard_negatives(sen_id=x[\"idx\"], n=n, dataset=\"val\"))\n","        test_ds = augmented_with_paraphrases[\"test\"].map(lambda x: self.get_hard_negatives(sen_id=x[\"idx\"], n=n, dataset=\"test\"))\n","\n","        # Combine the three datasets into one DatasetDict and return it\n","        return datasets.DatasetDict({\"train\": train_ds, \"validation\": val_ds, \"test\": test_ds})\n","\n","\n","\n","def paraphrase(parrot, row):\n","    '''\n","    :param parrot: Instance of Parrot paraphrase generator\n","    :param row: dataset row with one anchor sentence and one negative paraphrase sample\n","    :return:    dataset row with the anchor, one positive sample and the negative sample moved to column \"sen_2\"\n","    '''\n","    print(row[\"index\"])\n","    if row[\"label\"] == 1:\n","        row[\"sen_2\"] = \"\"\n","        return row\n","    row[\"sen_2\"] = row[\"sentence2\"]\n","    row[\"sen_1\"] = parrot.augment(row[\"sentence1\"])\n","    return row\n","\n","def generate_paraphrases(dataset: datasets.Dataset):\n","    \n","    #uncomment to get reproducable paraphrase generations\n","    def random_state(seed):\n","        torch.manual_seed(seed)\n","        if torch.cuda.is_available():\n","            torch.cuda.manual_seed_all(seed)\n","\n","        random_state(1234)\n","    \n","\n","    parrot = Parrot(model_tag=\"prithivida/parrot_paraphraser_on_T5\", use_gpu=False)\n","    out = []\n","    for row in dataset:\n","        out_row= paraphrase(parrot, row)\n","        out.append(out_row)\n","    #dataset = dataset.map(lambda x: paraphrase(parrot, x), num_proc=1)\n","\n","    return dataset\n","\n","\n","\n","\n","\n","\n","def construct_q_gram_set(string: str, q=3, q_padding=False):\n","    s = set()\n","\n","    # Apply padding if specified\n","    if q_padding:\n","        string = \"#\"*(q-1) + string + \"#\"*(q-1)\n","\n","    string = string.lower()\n","\n","    # If the string is shorter than Q, apply padding at the end\n","    if len(string) < q:\n","        string= string + str(\"#\"*(q-len(string)))\n","\n","    for i,_ in enumerate(string):\n","        q_gram = string[i:min(i+q, len(string))]\n","        if len(q_gram)==q:\n","            s.add(q_gram)\n","\n","    return s\n","\n","def construct_n_gram_set(s: str, all_n=[1, 2], filter_stopwords=False):\n","    out = set()\n","    s = s.lower()\n","    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n","\n","    tokens = [token for token in s.split(\" \") if token != \"\"]\n","    if filter_stopwords:\n","        tokens = [token for token in tokens if token not in stopwords.words('english')]\n","    for n in all_n:\n","         \n","        out |= set(ngrams(tokens, n))\n","    return out\n","\n","\n","class HardNegativeFinder():\n","    \"\"\"\n","    This class is used to find hard negative sentences for sentences in the provided dataset.\n","    These are written to Negative_Sentences.csv.\n","    \"\"\"\n","    def __init__(self, ds):\n","        self.train_ds = ds[ds[\"split\"] == \"train\"]\n","        self.val_ds = ds[ds[\"split\"] == \"validation\"]\n","        self.test_ds = ds[ds[\"split\"] == \"test\"]\n","\n","    def create_qgrams(self):\n","        \"\"\"\n","        For all positive pairs, only sentence 1 is turned into a Q-gram, because this is the anchor sentence.\n","        Then, both sentences of all negative pairs are also turned into Q-grams as matching candidates for the anchors.\n","\n","        In addition, all q-grams are saved to build the vocabulary used by LSH\n","        \"\"\"\n","        result_dict = {}\n","        loop_list = [{\"name\": \"train\", \"ds\": self.train_ds}, {\"name\": \"val\", \"ds\": self.val_ds},\n","                {\"name\": \"test\", \"ds\": self.test_ds}]\n","        print(\"Creating QGrams\")\n","        for pair in loop_list:\n","            vocab = set()\n","            anchor_sen = []\n","            anchor_idx = []\n","            candidate_sen = []\n","            candidate_idx = []\n","            for idx, row in pair[\"ds\"].iterrows():\n","\n","                # Get the Q-gram sets for all anchor sentences\n","                sen1 = row[\"anchor\"]\n","                #q_gram_set = construct_q_gram_set(sen1)\n","                q_gram_set =  construct_n_gram_set(sen1)\n","                #print(q_gram_set)\n","                # Add the set to the anchor and update the vocabulary\n","                anchor_sen.append(q_gram_set)\n","                vocab.update(q_gram_set)\n","\n","                # Add the idx to identify sentences\n","                anchor_idx.append(\"anchor_\" + str(idx))\n","\n","                candidate_sen.append(q_gram_set)\n","                candidate_idx.append(\"anchor_\" + str(idx))\n","\n","                # Depending on original label, add original Paraphrase or neg_1 to vocab\n","                sen2_label = \"para\" if row[\"original_label\"] == 1 else \"neg_1\"\n","                sen2 = row[sen2_label]\n","\n","                #q_gram_set2 = construct_q_gram_set(sen2)\n","                q_gram_set2 =  construct_n_gram_set(sen2)\n","\n","                # Add the sets to the candidates and the vocabulary\n","                candidate_sen.append(q_gram_set2)\n","                vocab.update(q_gram_set2)\n","\n","                # Add the idx to identify sentences\n","                candidate_idx.append(sen2_label + \"_\" + str(idx))\n","\n","\n","            # Create the Hasher for the current dataset based on the vocabulary and save the Q-grams\n","            Hasher = Locality_Sensitive_Hasher(vocab=vocab, num_signatures=200)\n","\n","            # Add both lists and the hasher to the result_dict\n","            result_dict[pair[\"name\"]] = {\"anchors\": {\"idx\": anchor_idx,\n","                                                    \"sen\": anchor_sen},\n","                                        \"candidates\": {\"idx\": candidate_idx,\n","                                                        \"sen\": candidate_sen},\n","                                        \"hasher\": Hasher}\n","\n","            self.q_gram_dict = result_dict\n","\n","\n","    def find_negatives(self):\n","        if not hasattr(self, \"q_gram_dict\"):\n","            self.create_qgrams()\n","\n","        # Get the signatures for the anchors and candidates for all three datasets\n","        self.get_signatures()\n","\n","        # Initialize a dictionary\n","        negatives_dict = {}\n","\n","        for ds_name, ds_dict in self.q_gram_dict.items():\n","\n","            print(\"\\n\\n\" + \"=\" * 50)\n","            print(f\"Forming matches for {ds_name}\")\n","            print(\"=\" * 50)\n","\n","            anchor_sig = ds_dict[\"anchors\"][\"sig\"]\n","            candidate_sig = ds_dict[\"candidates\"][\"sig\"]\n","            anchor_idx = ds_dict[\"anchors\"][\"idx\"]\n","            candidate_idx = ds_dict[\"candidates\"][\"idx\"]\n","\n","            # Initialize a sub dictionary for the current ds_name\n","            ds_match_dict = {}\n","\n","            # For each of the anchor signatures, find matches in the candidate signatures\n","            num_anchors = len(anchor_sig)\n","            for i, sig_vector in enumerate(anchor_sig):\n","                print(f\"- Anchor {i + 1}/{num_anchors}\")\n","\n","                # If an element in a row of candidate sig matches the element in sig_vector, its value is set to true\n","                # sum(axis=1) counts the number of True values per row\n","                match_vector = (candidate_sig==sig_vector).sum(axis=1)\n","\n","                # Combine the number of matches in match_vector with the candidate idx into a dictionary\n","                # Save the dict in the ds_match_dict (key: corresponding anchor idx)\n","                ds_match_dict[anchor_idx[i]] = dict(zip(candidate_idx, list(match_vector)))\n","            negatives_dict[ds_name] = ds_match_dict\n","\n","        # Set the negatives_dict attribute\n","        self.negatives_dict = negatives_dict\n","\n","\n","    def get_signatures(self):\n","        \"\"\"\n","        Uses the Hasher-instance in each sub-dictionary of self.q_gram_dict to create signature matrices for\n","        anchors and candidates.\n","        One row of the matrix corresponds to one sentence\n","        \"\"\"\n","        for ds_name, ds_dict in self.q_gram_dict.items():\n","\n","            print(\"\\n\\n\" + \"=\"*50)\n","            print(f\"Getting signatures for {ds_name}\")\n","            print(\"=\" * 50)\n","\n","            # Get the signature matrix for both the anchors and the candidates\n","            Hasher = ds_dict[\"hasher\"]\n","\n","            print(\"Anchor sentences:\")\n","            anchor_sig = Hasher.create_signature_matrix(ds_dict[\"anchors\"][\"sen\"])\n","\n","            print(\"\\nCandidate sentences:\")\n","            candidate_sig = Hasher.create_signature_matrix(ds_dict[\"candidates\"][\"sen\"])\n","\n","            # Add signatures to the dictionary\n","            ds_dict[\"anchors\"][\"sig\"] = anchor_sig\n","            ds_dict[\"candidates\"][\"sig\"] = candidate_sig\n","\n","\n","    def write_negatives(self, n=50, out_path=\"Negative_Sentences.csv\"):\n","        \"\"\"\n","        Function that uses the negatives identified in find_negatives and writes the sentences out to a csv-File\n","        Process per anchor sentence\n","        1. Sort the dictionary by value (number of matching signatures)\n","        2. Take the n sentences with the highest number of matches\n","        3. Identify the sentences in the dataset by their index and sentence number\n","        4. Write a new row into the CSV ([ds_type], [anchor_idx], [sentence1], [sentence2], ..., [sentence_N])\n","\n","        :param n:           Number of sentences to be stored per anchor\n","        :param out_path:    Path to the csv that stores the matches\n","        \"\"\"\n","\n","        if not hasattr(self, \"negatives_dict\"):\n","            self.find_negatives()\n","\n","        with open(out_path, \"w\") as file:\n","            sentence_columns = [(\"sen_\" + str(num)) for num in range(n)]\n","            header = \"dataset,anchor_idx,\" + \",\".join(sentence_columns) + \"\\n\"\n","            file.write(header)\n","\n","\n","        for ds_name, ds_dict in self.negatives_dict.items():\n","            for anchor_idx, num_match_dict in ds_dict.items():\n","                # 1. Get the candidate_idx as a list sorted by number of matches\n","                sorted_idx = list(dict(sorted(num_match_dict.items(), key=lambda x:x[1], reverse=True)).keys())\n","\n","                # 2. and 3. Get the first n sentences for the sorted_idx list\n","                top_n_sentences = self.identify_sentences(sorted_idx=sorted_idx, n=n, ds_name=ds_name) \n","\n","                # 4. Define the csv row and append it to the file\n","                csv_row = [ds_name, anchor_idx, *top_n_sentences]\n","\n","                with open(out_path, \"a\") as file:\n","                    writer = csv.writer(file)\n","                    writer.writerow(item for item in csv_row)\n","\n","\n","    def identify_sentences(self, sorted_idx: list, n, ds_name):\n","        \"\"\"\n","        Takes in a list of indices and a number of sentences.\n","        Identifies the sentences in the dataset and returns them as a list\n","        :param sorted_idx:      List of candidate indices; Sorted by number of matches with an anchor sentence\n","        :param n:               How many sentences to return\n","        :param ds_name:         Name of the dataset in which to look for the sentences\n","        :return:                List of the sentences with the highest number of matches\n","        \"\"\"\n","\n","        # Keep a maximum of n indices\n","        num_sentences = min(len(sorted_idx), n)\n","        sorted_idx = sorted_idx[:num_sentences]\n","\n","        # Turn the sentence indices into the correct format for the dataset\n","        # - The number at the end is the idx in the ds\n","        # - sen1 or sen2 identifies the sentence in the pair\n","        index_nums = [int(full_idx.split(\"_\")[-1]) for full_idx in sorted_idx]\n","        sen_keys = [\"para\" if full_idx.split(\"_\")[0] == \"para\" else \"neg_1\" for full_idx in sorted_idx]\n","\n","        # Filter the correct dataset to only contain sentences in the sorted_idx\n","        ds = getattr(self, ds_name + \"_ds\")\n","\n","        # Get the sentences based on their position in the ds and whether it is sentence1 or sentence2\n","        result_list = []\n","        for i, position in enumerate(index_nums):\n","            #sentence = ds[position][sen_keys[i]]\n","            sentence = ds.loc[position][sen_keys[i]]\n","            result_list.append(sentence)\n","\n","        return result_list\n","\n","\n","def create_vocab_dict(vocab):\n","    # Create a dictionary to map vocab elements to indices\n","    vocab_dict = {}\n","    for i, q_gram in enumerate(vocab):\n","        vocab_dict[q_gram] = i\n","\n","    return vocab_dict\n","\n","# ==============================================================\n","# LSH to speed up the comparison\n","# ==============================================================\n","class Locality_Sensitive_Hasher():\n","    def __init__(self, vocab, num_signatures=100):\n","        \"\"\"\n","        :param vocab:           Set of all q-grams in the sets to be encoded\n","        :param num_signatures:  How many signatures should be used to represent one q-gram set\n","        \"\"\"\n","\n","        self.vocab = vocab\n","        print('Setting vocabulary...')\n","        self.vocab_dict = create_vocab_dict(self.vocab)\n","        self.num_signatures = num_signatures\n","\n","        print('Creating hash functions...')\n","        self.update_lsh_hash_funcs()\n","\n","    def update_vocab(self, vocab):\n","        self.vocab = vocab\n","        self.vocab_dict = create_vocab_dict(self.vocab)\n","        print('Vocabulary updated')\n","\n","        print('Updating hash functions with new vocabulary...')\n","        self.update_lsh_hash_funcs()\n","\n","\n","    def update_lsh_hash_funcs(self):\n","        self.hash_list = []\n","        for i in range(self.num_signatures):\n","            # Create a randomized list of numbers from one to the length of the vocabulary\n","            hash_func = list(range(1, len(self.vocab) + 1))\n","            shuffle(hash_func)\n","            self.hash_list.append(hash_func)\n","\n","\n","    def create_signature_matrix(self, q_grams: list):\n","        \"\"\"\n","        Takes in a list of Q-grams and returns a matrix of signatures\n","        :param q_grams:     List of Q-grams\n","        :return:            Numpy array of signatures\n","        \"\"\"\n","\n","        num_elem = len(q_grams)\n","        sig_matrix = np.zeros((num_elem, self.num_signatures))\n","\n","        for i, q_gram in enumerate(q_grams):\n","            if i%20 == 0:\n","                print(f\"- Sentence {i+1}/{num_elem}\")\n","            sparse_vector = self.create_sparse_vector(q_gram)\n","            signature = self.create_dense_vector(sparse_vector)\n","\n","            sig_matrix[i:i+1, :] = signature\n","\n","        return sig_matrix\n","\n","\n","    def create_sparse_vector(self, q_gram_set):\n","        '''\n","        Takes in a string\n","        :param q_gram_set:      Set of q-grams to be turned into a sparse one-vector using self.vocab_dict\n","        :return:                One-hot vector corresponding to the provided value as numpy array\n","        '''\n","\n","        val_one_hot = np.zeros(len(self.vocab))\n","        # Identify the index of each q_gram in the dictionary and set the corresponding element to 1\n","        for q_gram in q_gram_set:\n","            ind = self.vocab_dict[q_gram]\n","            val_one_hot[ind:(ind + 1)] = 1\n","\n","        return val_one_hot\n","\n","\n","    def create_dense_vector(self, one_hot_vector):\n","        '''\n","        Creates a sparse signature vector using self.hash_list\n","        :param one_hot_vector:      Sparse one-hot vector to be turned into a dense signature vector\n","        :return:                    Dense signature vector as numpy array\n","        '''\n","        sig_vec = np.zeros(self.num_signatures, dtype=int)\n","        for sig_post, func in enumerate(self.hash_list):\n","            for i in range(1, len(self.vocab)+1):\n","                # Obtain the index of i in the hash-function (first iteration looks for the position of 1, then 2, ...)\n","                idx = func.index(i)\n","                vec_value = one_hot_vector[idx]\n","\n","                # If the value at that position in the vector is 1, append the signature value\n","                # and proceed with the next hash\n","                if vec_value==1:\n","                    sig_vec[sig_post:(sig_post+1)] = i\n","                    break\n","\n","        return sig_vec"],"metadata":{"id":"qKT3dEmvJhPd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from HardNegativeFinder import HardNegativeFinder\n","anchors_with_para = pd.read_csv(\"out_paws_train.csv\", index_col=\"index\", nrows=200)\n","nltk.download('stopwords')\n","finder = HardNegativeFinder(anchors_with_para)\n","\n","finder.find_negatives()\n","\n","finder.write_negatives(n=50, out_path=\"Negative_Sentences_paws_with_anchors.csv\")"],"metadata":{"id":"mSshsAdRmCK4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install python-terrier"],"metadata":{"id":"AOT78P2YYeh3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd '/content/drive/MyDrive/Uni Leipzig/SS2022/bdlt_contrastive/dataset'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WJ-4jsRrb03c","executionInfo":{"status":"ok","timestamp":1660234065001,"user_tz":-120,"elapsed":1004,"user":{"displayName":"Jonas Probst","userId":"12529974203254550175"}},"outputId":"61026e2e-169f-4ef5-996c-e5ebc1b5371e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Uni Leipzig/SS2022/bdlt_contrastive/dataset\n"]}]},{"cell_type":"code","source":["import pyterrier as pt\n","import pandas as pd\n","if not pt.started():\n","  pt.init()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AQ_j_pkbXpeR","executionInfo":{"status":"ok","timestamp":1660234067328,"user_tz":-120,"elapsed":1777,"user":{"displayName":"Jonas Probst","userId":"12529974203254550175"}},"outputId":"a50bb96d-2ba6-4134-929f-431d03e33a05"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["PyTerrier 0.8.1 has loaded Terrier 5.6 (built by craigmacdonald on 2021-09-17 13:27)\n","\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"OajIllrFbtIM","executionInfo":{"status":"ok","timestamp":1660234067329,"user_tz":-120,"elapsed":9,"user":{"displayName":"Jonas Probst","userId":"12529974203254550175"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["for split in [\"train\", \"validation\", \"test\"]:\n","    anchors_with_para = pd.read_csv(f\"out_[split}_paws.csv\", index_col=\"index\").fillna(\"\")\n","\n","    df = {\"docno\": [], \"text\": []}\n","    for index, row in anchors_with_para.iterrows():\n","        for sent in [\"anchor\", \"para\", \"neg_1\"]:\n","            if sent and sent.strip() !=\"\" and sent != \"None\":\n","                df[\"text\"].append(row[sent])\n","                df[\"docno\"].append(str(index) + \"_\" + sent)\n","    df = pd.DataFrame.from_dict(df)\n","    print(df)\n","    # index the text, record the docnos as metadata\n","    pd_indexer = pt.DFIndexer(f\"./pd_index_{split}\")\n","    indexref = pd_indexer.index(df[\"text\"], df[\"docno\"])\n","\n","    batch_ret = pt.BatchRetrieve(indexref,num_results=10)\n","    out_df = {\"anchor\":[], \"para\":[], \"neg1\":[], \"neg2\":[], \"neg3\":[], \"neg4\":[], \"neg5\":[], \"neg6\":[]}\n","    for idx_row,row in anchors_with_para.iterrows():\n","        out_df[\"anchor\"].append(row[\"anchor\"])\n","        out_df[\"para\"].append(row[\"para\"])\n","        \n","        if row[\"neg_1\"] != \"\":\n","            neg_count = 2\n","            out_df[\"neg1\"].append(row[\"neg_1\"])\n","        else:\n","            neg_count=1\n","\n","        s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', row[\"anchor\"])\n","\n","        res = batch_ret(s)\n","        if res.shape[0] < 10:\n","            print(res)\n","        for res_num ,r in res.iterrows():\n","            idx_num, sent = r[\"docno\"].split(\"_\", 1)[0],  r[\"docno\"].split(\"_\", 1)[1]\n","            if int(idx_num) != idx_row:\n","                out_df[f\"neg{neg_count}\"].append(anchors_with_para.loc[int(idx_num)][sent])\n","                neg_count += 1\n","            if neg_count == 7:\n","                break\n","        if neg_count < 7:\n","            for n in range(neg_count, 7):\n","                out_df[f\"neg{n}\"].append(\"\")\n","\n","    out_df = pd.DataFrame(out_df)\n","    out_df.to_csv(f\"out_{split}.csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1hNJCP_1bjOg","executionInfo":{"status":"ok","timestamp":1660234087546,"user_tz":-120,"elapsed":14047,"user":{"displayName":"Jonas Probst","userId":"12529974203254550175"}},"outputId":"0b5ad0e2-4f1f-4dc1-f16e-d2a520609772"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["            docno                                               text\n","0        0_anchor  In Paris , in October 1560 , he secretly met t...\n","1          0_para  In October 1560, he secretly met with English ...\n","2         0_neg_1  In October 1560 , he secretly met with the Eng...\n","3        1_anchor  The NBA season of 1975 -- 76 was the 30th seas...\n","4          1_para  The 1975 -- 76 season of the National Basketba...\n","...           ...                                                ...\n","5833    1998_para  Oconto is a village in Nebraska's Custer Count...\n","5834   1998_neg_1  Custer County , Nebraska , United States of Am...\n","5835  1999_anchor  In recognition of its history , its administra...\n","5836    1999_para  Cambridge was granted its city charter in 1951...\n","5837   1999_neg_1                                                   \n","\n","[5838 rows x 2 columns]\n","16:07:56.080 [main] WARN org.terrier.structures.indexing.Indexer - Adding an empty document to the index (1_neg_1) - further warnings are suppressed\n","16:08:04.678 [main] WARN org.terrier.structures.indexing.Indexer - Indexed 883 empty documents\n"]}]},{"cell_type":"code","source":["import re"],"metadata":{"id":"hHhG1ZGSp6vC","executionInfo":{"status":"ok","timestamp":1660234953724,"user_tz":-120,"elapsed":531,"user":{"displayName":"Jonas Probst","userId":"12529974203254550175"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QliKg7UzqGyL","executionInfo":{"status":"ok","timestamp":1660236221337,"user_tz":-120,"elapsed":22152,"user":{"displayName":"Jonas Probst","userId":"12529974203254550175"}},"outputId":"7af3e0e8-64dc-46c3-e4ac-db7bab04039c"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/pyterrier/transformer.py:269: FutureWarning: .transform() should be passed a dataframe. Use .search() to execute a single query.\n","  return self.transform(*args, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["  qid  docid        docno  rank      score  \\\n","0   1   2589   883_anchor     0  15.489986   \n","1   1   2591    883_neg_1     1  15.489986   \n","2   1   2590     883_para     2  13.074037   \n","3   1   5382  1844_anchor     3   2.687711   \n","4   1   5384   1844_neg_1     4   2.687711   \n","5   1   5383    1844_para     5   2.636588   \n","\n","                                               query  \n","0  Pidoux appeared as cellist Pablo Larra n in   ...  \n","1  Pidoux appeared as cellist Pablo Larra n in   ...  \n","2  Pidoux appeared as cellist Pablo Larra n in   ...  \n","3  Pidoux appeared as cellist Pablo Larra n in   ...  \n","4  Pidoux appeared as cellist Pablo Larra n in   ...  \n","5  Pidoux appeared as cellist Pablo Larra n in   ...  \n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/pyterrier/transformer.py:269: FutureWarning: .transform() should be passed a dataframe. Use .search() to execute a single query.\n","  return self.transform(*args, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["  qid  docid        docno  rank      score  \\\n","0   1   3975  1358_anchor     0  14.822788   \n","1   1   3976    1358_para     1  14.822788   \n","2   1   5502  1887_anchor     2   5.027579   \n","3   1   1167   395_anchor     3   4.528441   \n","4   1   1168     395_para     4   4.528441   \n","5   1   3429  1172_anchor     5   4.329036   \n","6   1   3430    1172_para     6   4.321682   \n","7   1   3431   1172_neg_1     7   4.321682   \n","\n","                                               query  \n","0  Yarde sometimes improvised   often while liste...  \n","1  Yarde sometimes improvised   often while liste...  \n","2  Yarde sometimes improvised   often while liste...  \n","3  Yarde sometimes improvised   often while liste...  \n","4  Yarde sometimes improvised   often while liste...  \n","5  Yarde sometimes improvised   often while liste...  \n","6  Yarde sometimes improvised   often while liste...  \n","7  Yarde sometimes improvised   often while liste...  \n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/pyterrier/transformer.py:269: FutureWarning: .transform() should be passed a dataframe. Use .search() to execute a single query.\n","  return self.transform(*args, **kwargs)\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"aKI7zgZ_uChJ","executionInfo":{"status":"ok","timestamp":1660236233431,"user_tz":-120,"elapsed":502,"user":{"displayName":"Jonas Probst","userId":"12529974203254550175"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["for key,value in out_df.items():\n","    print(len(value))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yN2lXtHGwmpz","executionInfo":{"status":"ok","timestamp":1660236026693,"user_tz":-120,"elapsed":613,"user":{"displayName":"Jonas Probst","userId":"12529974203254550175"}},"outputId":"a84f0286-505b-40cf-d966-dd0f7836626a"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["1946\n","1946\n","1946\n","1946\n","1946\n","1946\n","1945\n","1945\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["KSaEj8UXkjv8","fE3snvUmbw97"],"name":"DatasetGeneration.ipynb","provenance":[{"file_id":"113d6cexawrKtOsTdj5PqdewcSw-RjcOr","timestamp":1659969466566}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"aebe1edaa40448ec9606c4fc9512a35e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8b5547893d0544719b1785d27bed3952","IPY_MODEL_ba14bd923d8e4a65ae81712557f52e96","IPY_MODEL_8472ee5390914e2cb55d85728c8b7e98"],"layout":"IPY_MODEL_c66c70f66e374636ad1b6206bd76f98b"}},"8b5547893d0544719b1785d27bed3952":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_25caf84122e0457d8a8f46f9f4324cda","placeholder":"​","style":"IPY_MODEL_3e7910370d4f43b98db41ba43fea7d10","value":"100%"}},"ba14bd923d8e4a65ae81712557f52e96":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_08a39228f7334bdfa334bc0e2da9f4aa","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c5d306bdea5542d3981c353706395aec","value":3}},"8472ee5390914e2cb55d85728c8b7e98":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d9872e9704c8499fa0a0e4d2777caed3","placeholder":"​","style":"IPY_MODEL_d6664fddb2a1444d813500d58236663a","value":" 3/3 [00:00&lt;00:00,  9.67it/s]"}},"c66c70f66e374636ad1b6206bd76f98b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"25caf84122e0457d8a8f46f9f4324cda":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e7910370d4f43b98db41ba43fea7d10":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"08a39228f7334bdfa334bc0e2da9f4aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5d306bdea5542d3981c353706395aec":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d9872e9704c8499fa0a0e4d2777caed3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6664fddb2a1444d813500d58236663a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}