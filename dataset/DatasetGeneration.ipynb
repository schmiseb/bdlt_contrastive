{"cells":[{"cell_type":"markdown","source":["# 1. Preparation"],"metadata":{"id":"J_V6G4LUhwrA"}},{"cell_type":"markdown","source":["## 1.1 Mount drive"],"metadata":{"id":"KSaEj8UXkjv8"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"Uu8RafSARvNO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660230915229,"user_tz":-120,"elapsed":3834,"user":{"displayName":"Jonas Probst","userId":"12529974203254550175"}},"outputId":"18deb888-937f-4dd6-bd2c-150afdfa2eb8"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!cp -r /content/drive/MyDrive/Uni\\ Leipzig/SS2022/.ssh ~/"],"metadata":{"id":"PXf5iED29aeo","executionInfo":{"status":"ok","timestamp":1660230915229,"user_tz":-120,"elapsed":4,"user":{"displayName":"Jonas Probst","userId":"12529974203254550175"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fE3snvUmbw97"},"source":["## 1.2 Get correct CuDNN version\n","See https://github.com/googlecolab/colabtools/issues/2600\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-pbxWrx2eUE2"},"outputs":[],"source":["!apt install --allow-change-held-packages libcudnn8=8.4.1.50-1+cuda11.6"]},{"cell_type":"markdown","metadata":{"id":"pO9PjgdF-cXV"},"source":["## 1.3 Install necessary packages"]},{"cell_type":"code","source":["%cd '/content/drive/MyDrive/Uni Leipzig/SS2022/bdlt_contrastive/dataset'"],"metadata":{"id":"KlyQDWNmLcK1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install git+https://github.com/PrithivirajDamodaran/Parrot_Paraphraser.git"],"metadata":{"id":"-KCxApQZ5RdB"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ui8nCZIa-bvM"},"outputs":[],"source":["!python3 -m pip install -r '../requirements.txt'"]},{"cell_type":"code","source":["import re\n","\n","def para(model, tokenizer, row, anchor_col=\"sentence1\", para_col=\"sentence2\", label_col=\"label\", path=\"glue\", name=\"mrpc\", split=\"train\"):\n","  out_row = {\n","             \"anchor\": row[anchor_col],\n","             \"para\": row[para_col],\n","             \"neg_1\": \"\",\n","             \"original_label\": row[label_col],\n","             \"contains_numeric\": False,\n","             \"path\": path,\n","             \"name\": name,\n","             \"split\": split}\n","\n","  # return samples which contain paraphrase as is\n","  if row[label_col] == 1:\n","        return out_row\n","\n","\n","  # move hard negative to neg_1\n","  out_row[\"neg_1\"] = row[para_col]\n","  #mark rows with digits in anchor sentence, leads to logical errors in paraphrase\n","  if re.search(\"\\d\", row[anchor_col]):\n","        out_row[\"contains_numeric\"] = True\n","\n","\n","  anchor = row[anchor_col]\n","  anchor = re.sub(r'\\s([\\.,!\\?\\'\"])', r'\\1', anchor)\n","  anchor = anchor.replace('\" ', '\"')\n","\n","\n","  encoding = tokenizer.encode(\"paraphrase: \" + anchor, return_tensors=\"pt\")\n","  input_ids = encoding.to(device)\n","\n","\n","  paraphrases = model.generate(\n","      input_ids=input_ids,\n","      max_length=128,\n","      early_stopping=True,\n","      num_beams=3,\n","      num_return_sequences=1\n","\n","  )\n","  \t\n","  para = tokenizer.decode(paraphrases[0], skip_special_tokens=True,clean_up_tokenization_spaces=True)\n","\n","  # remove first tokens from anchor and paraphrase, that contain \"paraphrsedoutput: \" and \"paraphrase: \"\n","  para_token_list =paraphrases[0].tolist()[7:]\n","  input_token_list = input_ids.tolist()[0][3:]\n","\n","  # check if generated tokens are the same as in original sentence, which means sentence was not altered or just shuffeled.\n","  # Sentences like this are not used\n","  # remove paraphrases without new tokens and empty ones\n","  if not para or sorted(para_token_list) ==sorted(input_token_list) or len(para_token_list) == 128:\n","    out_row[\"para\"] = \"None\"\n","    return out_row\n","  else:\n","      para = para.replace(\"paraphrasedoutput: \", \"\")\n","      out_row[\"para\"] = para\n","    \n","  return out_row"],"metadata":{"id":"hCgATXCZXoze"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","import torch\n","import datasets\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = AutoModelForSeq2SeqLM.from_pretrained(\"ramsrigouthamg/t5-large-paraphraser-diverse-high-quality\")\n","tokenizer = AutoTokenizer.from_pretrained(\"ramsrigouthamg/t5-large-paraphraser-diverse-high-quality\")\n","model.to(device)\n","print(\" \")\n"],"metadata":{"id":"_sFVm3Wu6w75"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","dataset_definitions = [\n","    #{\"path\": \"glue\", \"name\": \"mrpc\"},\n","    {\"path\": \"paws\", \"name\": \"labeled_final\"},\n","    #{\"path\": \"paws\", \"name\": \"labeled_swap\"},\n","    #{\"path\": \"paws\", \"name\": \"unlabeled_final\"}\n","]\n","out = []\n","for definition in dataset_definitions:\n","  dataset = datasets.load_dataset(path=definition[\"path\"], name=definition[\"name\"])\n","  for split in [\"train\"]: #, \"validation\", \"test\"]:\n","    if split not in dataset:\n","      continue\n","    dataset_part = pd.DataFrame(dataset[split].select(range(2000)))\n","    #dataset_post = dataset_part.map(lambda x: para(model, tokenizer, x), num_proc=1)\n","    for idx, row in dataset_part.iterrows():\n","      out.append(para(model, tokenizer,row, split=split, path=definition[\"path\"], name=definition[\"name\"]))\n","out = pd.DataFrame(out)\n","out = out[out[\"para\"] != \"None\"]\n","out = out.reset_index()\n","out.to_csv(\"out_paws_train.csv\", index=False)"],"metadata":{"id":"77mHxTyfiIwq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import datasets\n","import pandas as pd\n","import csv\n","import numpy as np\n","from random import shuffle\n","from parrot import Parrot\n","import torch\n","from nltk.util import ngrams\n","import nltk\n","from nltk.corpus import stopwords\n","\n","# =============================================================\n","# Hard negative mining\n","# =============================================================\n","class HardNegativePreparer():\n","    \"\"\"\n","    This class is used by the contrastive learning_manager to get negative samples for specified sentences.\n","    It takes the sentences from the csv \"Negative_Sentences.csv\" created by the HardNegativeFinder\n","    \"\"\"\n","    def __init__(self, csv_path=\"Negative_Sentences_Sig300.csv\"):\n","        self.df = pd.read_csv(csv_path)\n","\n","    def build_dataset_with_negatives(self, dataset: datasets.Dataset, n=1):\n","        \"\"\"\n","        :param dataset: Dataset with two sentences and a label\n","        :param n:       Number of negatives to add\n","        :return:        Dataset that only contains the pairs that have a label of 1.\n","                        Additionally, n more sentences were added\n","        \"\"\"\n","        # Filter only positive samples\n","        filtered_ds = dataset.filter(lambda x: x[\"label\"] == 1)\n","\n","        # Add the hard negatives to each ds separately\n","        train_ds = filtered_ds[\"train\"].map(lambda x: self.get_hard_negatives(sen_id=x[\"idx\"], n=n, dataset=\"train\"))\n","        val_ds = filtered_ds[\"validation\"].map(lambda x: self.get_hard_negatives(sen_id=x[\"idx\"], n=n, dataset=\"val\"))\n","        test_ds = filtered_ds[\"test\"].map(lambda x: self.get_hard_negatives(sen_id=x[\"idx\"], n=n, dataset=\"test\"))\n","\n","        # Combine the three datasets into one DatasetDict and return it\n","        return datasets.DatasetDict({\"train\": train_ds, \"validation\": val_ds, \"test\": test_ds})\n","\n","    def get_hard_negatives(self, sen_id: int, n=1, dataset=\"train\"):\n","        \"\"\"\n","        Takes in a sentence i and returns n hard negatives as obtained by the HardNegativeFinder\n","        :param sen_id:  The id for which to return hard negatives\n","        :param n:       The number of hard negatives to return\n","        :param dataset: The name of the dataset the sentence belongs to\n","        :return:        A dictionary of hard negative sentences (starting with sentence3, sentence4, ...)\n","        \"\"\"\n","     \n","        # Define the columns of self.df based on n\n","        columns = [str(\"sen_\" + str(idx)) for idx in range(n)]\n","\n","        # Get the correct row by matching the sen_id on anchor_idx and the dataset name\n","        row = self.df.loc[((self.df[\"anchor_idx\"] == \"sen1_\" + str(sen_id)) & (self.df[\"dataset\"]==dataset))]\n","\n","        # Get the sentences as a list and turn them into a dictionary starting with key sentence3\n","        sentences = row[columns].values.tolist()[0]\n","        result_dict = dict(zip([\"sentence\" + str(idx + 3) for idx in range(n)], sentences))\n","        return result_dict\n","    \n","\n","    def build_dataset_with_positives_and_negatives(self, dataset: datasets.Dataset, n=1):\n","        \"\"\"\n","        :param dataset: Dataset with two sentences and a label\n","        :param n:       Number of negatives to add\n","        :return:        Augmented Dataset. Negative labeled Sentences get one positive paraphrase added and are filled to reach n negatives\n","                        For already positive pairs, only negatives are created.\n","        \"\"\"\n","        # Filter only negative samples, for which positive is created\n","        #filtered_ds = dataset.filter(lambda x: x[\"label\"] == 0)\n","\n","        augmented_with_paraphrases = generate_paraphrases(dataset)\n","        augmented_with_paraphrases.to_csv(\"dataset_with_para.csv\")\n","        return\n","\n","\n","        # Add the hard negatives to each ds separately\n","        train_ds = augmented_with_paraphrases[\"train\"].map(lambda x: self.get_hard_negatives(sen_id=x[\"idx\"], n=n, dataset=\"train\"))\n","        val_ds = augmented_with_paraphrases[\"validation\"].map(lambda x: self.get_hard_negatives(sen_id=x[\"idx\"], n=n, dataset=\"val\"))\n","        test_ds = augmented_with_paraphrases[\"test\"].map(lambda x: self.get_hard_negatives(sen_id=x[\"idx\"], n=n, dataset=\"test\"))\n","\n","        # Combine the three datasets into one DatasetDict and return it\n","        return datasets.DatasetDict({\"train\": train_ds, \"validation\": val_ds, \"test\": test_ds})\n","\n","\n","\n","def paraphrase(parrot, row):\n","    '''\n","    :param parrot: Instance of Parrot paraphrase generator\n","    :param row: dataset row with one anchor sentence and one negative paraphrase sample\n","    :return:    dataset row with the anchor, one positive sample and the negative sample moved to column \"sen_2\"\n","    '''\n","    print(row[\"index\"])\n","    if row[\"label\"] == 1:\n","        row[\"sen_2\"] = \"\"\n","        return row\n","    row[\"sen_2\"] = row[\"sentence2\"]\n","    row[\"sen_1\"] = parrot.augment(row[\"sentence1\"])\n","    return row\n","\n","def generate_paraphrases(dataset: datasets.Dataset):\n","    \n","    #uncomment to get reproducable paraphrase generations\n","    def random_state(seed):\n","        torch.manual_seed(seed)\n","        if torch.cuda.is_available():\n","            torch.cuda.manual_seed_all(seed)\n","\n","        random_state(1234)\n","    \n","\n","    parrot = Parrot(model_tag=\"prithivida/parrot_paraphraser_on_T5\", use_gpu=False)\n","    out = []\n","    for row in dataset:\n","        out_row= paraphrase(parrot, row)\n","        out.append(out_row)\n","    #dataset = dataset.map(lambda x: paraphrase(parrot, x), num_proc=1)\n","\n","    return dataset\n","\n","\n","\n","\n","\n","\n","def construct_q_gram_set(string: str, q=3, q_padding=False):\n","    s = set()\n","\n","    # Apply padding if specified\n","    if q_padding:\n","        string = \"#\"*(q-1) + string + \"#\"*(q-1)\n","\n","    string = string.lower()\n","\n","    # If the string is shorter than Q, apply padding at the end\n","    if len(string) < q:\n","        string= string + str(\"#\"*(q-len(string)))\n","\n","    for i,_ in enumerate(string):\n","        q_gram = string[i:min(i+q, len(string))]\n","        if len(q_gram)==q:\n","            s.add(q_gram)\n","\n","    return s\n","\n","def construct_n_gram_set(s: str, all_n=[1, 2], filter_stopwords=False):\n","    out = set()\n","    s = s.lower()\n","    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n","\n","    tokens = [token for token in s.split(\" \") if token != \"\"]\n","    if filter_stopwords:\n","        tokens = [token for token in tokens if token not in stopwords.words('english')]\n","    for n in all_n:\n","         \n","        out |= set(ngrams(tokens, n))\n","    return out\n","\n","\n","class HardNegativeFinder():\n","    \"\"\"\n","    This class is used to find hard negative sentences for sentences in the provided dataset.\n","    These are written to Negative_Sentences.csv.\n","    \"\"\"\n","    def __init__(self, ds):\n","        self.train_ds = ds[ds[\"split\"] == \"train\"]\n","        self.val_ds = ds[ds[\"split\"] == \"validation\"]\n","        self.test_ds = ds[ds[\"split\"] == \"test\"]\n","\n","    def create_qgrams(self):\n","        \"\"\"\n","        For all positive pairs, only sentence 1 is turned into a Q-gram, because this is the anchor sentence.\n","        Then, both sentences of all negative pairs are also turned into Q-grams as matching candidates for the anchors.\n","\n","        In addition, all q-grams are saved to build the vocabulary used by LSH\n","        \"\"\"\n","        result_dict = {}\n","        loop_list = [{\"name\": \"train\", \"ds\": self.train_ds}, {\"name\": \"val\", \"ds\": self.val_ds},\n","                {\"name\": \"test\", \"ds\": self.test_ds}]\n","        print(\"Creating QGrams\")\n","        for pair in loop_list:\n","            vocab = set()\n","            anchor_sen = []\n","            anchor_idx = []\n","            candidate_sen = []\n","            candidate_idx = []\n","            for idx, row in pair[\"ds\"].iterrows():\n","\n","                # Get the Q-gram sets for all anchor sentences\n","                sen1 = row[\"anchor\"]\n","                #q_gram_set = construct_q_gram_set(sen1)\n","                q_gram_set =  construct_n_gram_set(sen1)\n","                #print(q_gram_set)\n","                # Add the set to the anchor and update the vocabulary\n","                anchor_sen.append(q_gram_set)\n","                vocab.update(q_gram_set)\n","\n","                # Add the idx to identify sentences\n","                anchor_idx.append(\"anchor_\" + str(idx))\n","\n","                candidate_sen.append(q_gram_set)\n","                candidate_idx.append(\"anchor_\" + str(idx))\n","\n","                # Depending on original label, add original Paraphrase or neg_1 to vocab\n","                sen2_label = \"para\" if row[\"original_label\"] == 1 else \"neg_1\"\n","                sen2 = row[sen2_label]\n","\n","                #q_gram_set2 = construct_q_gram_set(sen2)\n","                q_gram_set2 =  construct_n_gram_set(sen2)\n","\n","                # Add the sets to the candidates and the vocabulary\n","                candidate_sen.append(q_gram_set2)\n","                vocab.update(q_gram_set2)\n","\n","                # Add the idx to identify sentences\n","                candidate_idx.append(sen2_label + \"_\" + str(idx))\n","\n","\n","            # Create the Hasher for the current dataset based on the vocabulary and save the Q-grams\n","            Hasher = Locality_Sensitive_Hasher(vocab=vocab, num_signatures=200)\n","\n","            # Add both lists and the hasher to the result_dict\n","            result_dict[pair[\"name\"]] = {\"anchors\": {\"idx\": anchor_idx,\n","                                                    \"sen\": anchor_sen},\n","                                        \"candidates\": {\"idx\": candidate_idx,\n","                                                        \"sen\": candidate_sen},\n","                                        \"hasher\": Hasher}\n","\n","            self.q_gram_dict = result_dict\n","\n","\n","    def find_negatives(self):\n","        if not hasattr(self, \"q_gram_dict\"):\n","            self.create_qgrams()\n","\n","        # Get the signatures for the anchors and candidates for all three datasets\n","        self.get_signatures()\n","\n","        # Initialize a dictionary\n","        negatives_dict = {}\n","\n","        for ds_name, ds_dict in self.q_gram_dict.items():\n","\n","            print(\"\\n\\n\" + \"=\" * 50)\n","            print(f\"Forming matches for {ds_name}\")\n","            print(\"=\" * 50)\n","\n","            anchor_sig = ds_dict[\"anchors\"][\"sig\"]\n","            candidate_sig = ds_dict[\"candidates\"][\"sig\"]\n","            anchor_idx = ds_dict[\"anchors\"][\"idx\"]\n","            candidate_idx = ds_dict[\"candidates\"][\"idx\"]\n","\n","            # Initialize a sub dictionary for the current ds_name\n","            ds_match_dict = {}\n","\n","            # For each of the anchor signatures, find matches in the candidate signatures\n","            num_anchors = len(anchor_sig)\n","            for i, sig_vector in enumerate(anchor_sig):\n","                print(f\"- Anchor {i + 1}/{num_anchors}\")\n","\n","                # If an element in a row of candidate sig matches the element in sig_vector, its value is set to true\n","                # sum(axis=1) counts the number of True values per row\n","                match_vector = (candidate_sig==sig_vector).sum(axis=1)\n","\n","                # Combine the number of matches in match_vector with the candidate idx into a dictionary\n","                # Save the dict in the ds_match_dict (key: corresponding anchor idx)\n","                ds_match_dict[anchor_idx[i]] = dict(zip(candidate_idx, list(match_vector)))\n","            negatives_dict[ds_name] = ds_match_dict\n","\n","        # Set the negatives_dict attribute\n","        self.negatives_dict = negatives_dict\n","\n","\n","    def get_signatures(self):\n","        \"\"\"\n","        Uses the Hasher-instance in each sub-dictionary of self.q_gram_dict to create signature matrices for\n","        anchors and candidates.\n","        One row of the matrix corresponds to one sentence\n","        \"\"\"\n","        for ds_name, ds_dict in self.q_gram_dict.items():\n","\n","            print(\"\\n\\n\" + \"=\"*50)\n","            print(f\"Getting signatures for {ds_name}\")\n","            print(\"=\" * 50)\n","\n","            # Get the signature matrix for both the anchors and the candidates\n","            Hasher = ds_dict[\"hasher\"]\n","\n","            print(\"Anchor sentences:\")\n","            anchor_sig = Hasher.create_signature_matrix(ds_dict[\"anchors\"][\"sen\"])\n","\n","            print(\"\\nCandidate sentences:\")\n","            candidate_sig = Hasher.create_signature_matrix(ds_dict[\"candidates\"][\"sen\"])\n","\n","            # Add signatures to the dictionary\n","            ds_dict[\"anchors\"][\"sig\"] = anchor_sig\n","            ds_dict[\"candidates\"][\"sig\"] = candidate_sig\n","\n","\n","    def write_negatives(self, n=50, out_path=\"Negative_Sentences.csv\"):\n","        \"\"\"\n","        Function that uses the negatives identified in find_negatives and writes the sentences out to a csv-File\n","        Process per anchor sentence\n","        1. Sort the dictionary by value (number of matching signatures)\n","        2. Take the n sentences with the highest number of matches\n","        3. Identify the sentences in the dataset by their index and sentence number\n","        4. Write a new row into the CSV ([ds_type], [anchor_idx], [sentence1], [sentence2], ..., [sentence_N])\n","\n","        :param n:           Number of sentences to be stored per anchor\n","        :param out_path:    Path to the csv that stores the matches\n","        \"\"\"\n","\n","        if not hasattr(self, \"negatives_dict\"):\n","            self.find_negatives()\n","\n","        with open(out_path, \"w\") as file:\n","            sentence_columns = [(\"sen_\" + str(num)) for num in range(n)]\n","            header = \"dataset,anchor_idx,\" + \",\".join(sentence_columns) + \"\\n\"\n","            file.write(header)\n","\n","\n","        for ds_name, ds_dict in self.negatives_dict.items():\n","            for anchor_idx, num_match_dict in ds_dict.items():\n","                # 1. Get the candidate_idx as a list sorted by number of matches\n","                sorted_idx = list(dict(sorted(num_match_dict.items(), key=lambda x:x[1], reverse=True)).keys())\n","\n","                # 2. and 3. Get the first n sentences for the sorted_idx list\n","                top_n_sentences = self.identify_sentences(sorted_idx=sorted_idx, n=n, ds_name=ds_name) \n","\n","                # 4. Define the csv row and append it to the file\n","                csv_row = [ds_name, anchor_idx, *top_n_sentences]\n","\n","                with open(out_path, \"a\") as file:\n","                    writer = csv.writer(file)\n","                    writer.writerow(item for item in csv_row)\n","\n","\n","    def identify_sentences(self, sorted_idx: list, n, ds_name):\n","        \"\"\"\n","        Takes in a list of indices and a number of sentences.\n","        Identifies the sentences in the dataset and returns them as a list\n","        :param sorted_idx:      List of candidate indices; Sorted by number of matches with an anchor sentence\n","        :param n:               How many sentences to return\n","        :param ds_name:         Name of the dataset in which to look for the sentences\n","        :return:                List of the sentences with the highest number of matches\n","        \"\"\"\n","\n","        # Keep a maximum of n indices\n","        num_sentences = min(len(sorted_idx), n)\n","        sorted_idx = sorted_idx[:num_sentences]\n","\n","        # Turn the sentence indices into the correct format for the dataset\n","        # - The number at the end is the idx in the ds\n","        # - sen1 or sen2 identifies the sentence in the pair\n","        index_nums = [int(full_idx.split(\"_\")[-1]) for full_idx in sorted_idx]\n","        sen_keys = [\"para\" if full_idx.split(\"_\")[0] == \"para\" else \"neg_1\" for full_idx in sorted_idx]\n","\n","        # Filter the correct dataset to only contain sentences in the sorted_idx\n","        ds = getattr(self, ds_name + \"_ds\")\n","\n","        # Get the sentences based on their position in the ds and whether it is sentence1 or sentence2\n","        result_list = []\n","        for i, position in enumerate(index_nums):\n","            #sentence = ds[position][sen_keys[i]]\n","            sentence = ds.loc[position][sen_keys[i]]\n","            result_list.append(sentence)\n","\n","        return result_list\n","\n","\n","def create_vocab_dict(vocab):\n","    # Create a dictionary to map vocab elements to indices\n","    vocab_dict = {}\n","    for i, q_gram in enumerate(vocab):\n","        vocab_dict[q_gram] = i\n","\n","    return vocab_dict\n","\n","# ==============================================================\n","# LSH to speed up the comparison\n","# ==============================================================\n","class Locality_Sensitive_Hasher():\n","    def __init__(self, vocab, num_signatures=100):\n","        \"\"\"\n","        :param vocab:           Set of all q-grams in the sets to be encoded\n","        :param num_signatures:  How many signatures should be used to represent one q-gram set\n","        \"\"\"\n","\n","        self.vocab = vocab\n","        print('Setting vocabulary...')\n","        self.vocab_dict = create_vocab_dict(self.vocab)\n","        self.num_signatures = num_signatures\n","\n","        print('Creating hash functions...')\n","        self.update_lsh_hash_funcs()\n","\n","    def update_vocab(self, vocab):\n","        self.vocab = vocab\n","        self.vocab_dict = create_vocab_dict(self.vocab)\n","        print('Vocabulary updated')\n","\n","        print('Updating hash functions with new vocabulary...')\n","        self.update_lsh_hash_funcs()\n","\n","\n","    def update_lsh_hash_funcs(self):\n","        self.hash_list = []\n","        for i in range(self.num_signatures):\n","            # Create a randomized list of numbers from one to the length of the vocabulary\n","            hash_func = list(range(1, len(self.vocab) + 1))\n","            shuffle(hash_func)\n","            self.hash_list.append(hash_func)\n","\n","\n","    def create_signature_matrix(self, q_grams: list):\n","        \"\"\"\n","        Takes in a list of Q-grams and returns a matrix of signatures\n","        :param q_grams:     List of Q-grams\n","        :return:            Numpy array of signatures\n","        \"\"\"\n","\n","        num_elem = len(q_grams)\n","        sig_matrix = np.zeros((num_elem, self.num_signatures))\n","\n","        for i, q_gram in enumerate(q_grams):\n","            if i%20 == 0:\n","                print(f\"- Sentence {i+1}/{num_elem}\")\n","            sparse_vector = self.create_sparse_vector(q_gram)\n","            signature = self.create_dense_vector(sparse_vector)\n","\n","            sig_matrix[i:i+1, :] = signature\n","\n","        return sig_matrix\n","\n","\n","    def create_sparse_vector(self, q_gram_set):\n","        '''\n","        Takes in a string\n","        :param q_gram_set:      Set of q-grams to be turned into a sparse one-vector using self.vocab_dict\n","        :return:                One-hot vector corresponding to the provided value as numpy array\n","        '''\n","\n","        val_one_hot = np.zeros(len(self.vocab))\n","        # Identify the index of each q_gram in the dictionary and set the corresponding element to 1\n","        for q_gram in q_gram_set:\n","            ind = self.vocab_dict[q_gram]\n","            val_one_hot[ind:(ind + 1)] = 1\n","\n","        return val_one_hot\n","\n","\n","    def create_dense_vector(self, one_hot_vector):\n","        '''\n","        Creates a sparse signature vector using self.hash_list\n","        :param one_hot_vector:      Sparse one-hot vector to be turned into a dense signature vector\n","        :return:                    Dense signature vector as numpy array\n","        '''\n","        sig_vec = np.zeros(self.num_signatures, dtype=int)\n","        for sig_post, func in enumerate(self.hash_list):\n","            for i in range(1, len(self.vocab)+1):\n","                # Obtain the index of i in the hash-function (first iteration looks for the position of 1, then 2, ...)\n","                idx = func.index(i)\n","                vec_value = one_hot_vector[idx]\n","\n","                # If the value at that position in the vector is 1, append the signature value\n","                # and proceed with the next hash\n","                if vec_value==1:\n","                    sig_vec[sig_post:(sig_post+1)] = i\n","                    break\n","\n","        return sig_vec"],"metadata":{"id":"qKT3dEmvJhPd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from HardNegativeFinder import HardNegativeFinder\n","anchors_with_para = pd.read_csv(\"out_paws_train.csv\", index_col=\"index\", nrows=200)\n","nltk.download('stopwords')\n","finder = HardNegativeFinder(anchors_with_para)\n","\n","finder.find_negatives()\n","\n","finder.write_negatives(n=50, out_path=\"Negative_Sentences_paws_with_anchors.csv\")"],"metadata":{"id":"mSshsAdRmCK4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install python-terrier"],"metadata":{"id":"AOT78P2YYeh3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd '/content/drive/MyDrive/Uni Leipzig/SS2022/bdlt_contrastive/dataset'"],"metadata":{"id":"WJ-4jsRrb03c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pyterrier as pt\n","import pandas as pd\n","if not pt.started():\n","  pt.init()"],"metadata":{"id":"AQ_j_pkbXpeR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"OajIllrFbtIM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["anchors_with_para = pd.read_csv(\"out_paws_train.csv\", index_col=\"index\", nrows=20)\n","\n","df = {\"docno\": [], \"text\": []}\n","\n","for index, row in anchors_with_para.iterrows():\n","    for sent in [\"anchor\", \"para\", \"neg_1\"]:\n","        if sent !=\"\" and sent != \"None\":\n","            df[\"text\"].append(row[sent])\n","            df[\"docno\"].append(str(index) + \"_\" + sent)\n","df = pd.DataFrame.from_dict(df)\n","# index the text, record the docnos as metadata\n","pd_indexer = pt.DFIndexer(\"./pd_index\")\n","indexref = pd_indexer.index(df[\"text\"], df[\"docno\"])"],"metadata":{"id":"1hNJCP_1bjOg"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["KSaEj8UXkjv8","fE3snvUmbw97"],"name":"DatasetGeneration.ipynb","provenance":[{"file_id":"113d6cexawrKtOsTdj5PqdewcSw-RjcOr","timestamp":1659969466566}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}